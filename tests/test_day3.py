# tests/test_day3.py
import os
from app.services.rag.ingest import RAGIngestor
from app.services.rag.rag_pipeline import RAGPipeline

DOC_DIR = "data/docs"
os.makedirs(DOC_DIR, exist_ok=True)

# create a sample document
sample_path = os.path.join(DOC_DIR, "zennial_sample.txt")
sample_text = """
Zennial AI System Documentation

System Architecture:
The system uses a FastAPI backend (optional), LangGraph workflows for orchestration, and a memory layer
built on ChromaDB for long-term storage. Documents are chunked and embedded with sentence-transformers,
then stored in a RAG collection.

Installation:
1. Create conda env with Python 3.11
2. Install dependencies from requirements.txt
3. Run ingestion to index docs

Usage:
Ask questions like "What does the system architecture say?" and the RAG pipeline will return relevant snippets
and an answer generated by Gemini.
"""

with open(sample_path, "w", encoding="utf-8") as f:
    f.write(sample_text.strip())

# Ingest the document
ingestor = RAGIngestor()
inserted = ingestor.ingest_file(sample_path)
print("Inserted chunks:", len(inserted))

# Run RAG query
pipeline = RAGPipeline()
res = pipeline.answer("What does the system architecture say?", top_k=3)

print("\nRetrieved chunks metadata:")
for r in res["retrieved"]:
    print("-", r["metadata"])

print("\nAnswer / LLM output:\n", res["answer"])
